{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7080a470-8c29-4102-8090-a6cfc2a06f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "U≈ºyj ka≈ºdƒÖ z tych funkcji \n",
    "* `unix_timestamp()` \n",
    "* `date_format()`\n",
    "* `to_unix_timestamp()`\n",
    "* `from_unixtime()`\n",
    "* `to_date()` \n",
    "* `to_timestamp()` \n",
    "* `from_utc_timestamp()` \n",
    "* `to_utc_timestamp()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9776f7-6073-41f0-8f38-0490f0788c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+---------+------------+--------------------+\n",
      "|          timestamp|         unix|     Date|current_date|   current_timestamp|\n",
      "+-------------------+-------------+---------+------------+--------------------+\n",
      "|2015-03-22T14:13:34|1646641525847|May, 2021|  2025-03-12|2025-03-12 16:28:...|\n",
      "|2015-03-22T15:03:18|1646641557555|Mar, 2021|  2025-03-12|2025-03-12 16:28:...|\n",
      "|2015-03-22T14:38:39|1646641578622|Jan, 2021|  2025-03-12|2025-03-12 16:28:...|\n",
      "+-------------------+-------------+---------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "# üîπ Inicjalizacja SparkSession (to jest najwa≈ºniejsze!)\n",
    "spark = SparkSession.builder.appName(\"TestApp\").getOrCreate()\n",
    "\n",
    "# üîπ Dane\n",
    "kolumny = [\"timestamp\", \"unix\", \"Date\"]\n",
    "dane = [\n",
    "    (\"2015-03-22T14:13:34\", 1646641525847, \"May, 2021\"),\n",
    "    (\"2015-03-22T15:03:18\", 1646641557555, \"Mar, 2021\"),\n",
    "    (\"2015-03-22T14:38:39\", 1646641578622, \"Jan, 2021\")\n",
    "]\n",
    "\n",
    "# üîπ Tworzenie DataFrame\n",
    "dataFrame = spark.createDataFrame(dane, kolumny) \\\n",
    "    .withColumn(\"current_date\", current_date()) \\\n",
    "    .withColumn(\"current_timestamp\", current_timestamp())\n",
    "\n",
    "# üîπ Wy≈õwietlenie danych\n",
    "dataFrame.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc884ee-94a0-456b-9212-48b7bcaac341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- unix: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0f5676-af9a-4c46-9e90-b4270f30f222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## unix_timestamp(..) & cast(..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da2c85c6-dc7c-4519-a586-411a8fc3a0ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Konwersja **string** to a **timestamp**.\n",
    "\n",
    "Lokalizacja funkcji \n",
    "* `pyspark.sql.functions` in the case of Python\n",
    "* `org.apache.spark.sql.functions` in the case of Scala & Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29619a12-2790-4d54-bd50-abf02531a3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Zmiana formatu warto≈õci timestamp yyyy-MM-dd'T'HH:mm:ss \n",
    "`unix_timestamp(..)`\n",
    "\n",
    "Dokumentacja API `unix_timestamp(..)`:\n",
    "> Convert time string with given pattern (see <a href=\"http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\" target=\"_blank\">SimpleDateFormat</a>) to Unix time stamp (in seconds), return null if fail.\n",
    "\n",
    "`SimpleDataFormat` is part of the Java API and provides support for parsing and formatting date and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a88bca0-2ce7-4a7b-b332-d53e84fe79ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- unix: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      " |-- unix_timestamp: long (nullable = true)\n",
      "\n",
      "+-------------------+-------------+---------+------------+--------------------+--------------+\n",
      "|          timestamp|         unix|     Date|current_date|   current_timestamp|unix_timestamp|\n",
      "+-------------------+-------------+---------+------------+--------------------+--------------+\n",
      "|2015-03-22T14:13:34|1646641525847|May, 2021|  2025-03-12|2025-03-12 16:29:...|    1427030014|\n",
      "|2015-03-22T15:03:18|1646641557555|Mar, 2021|  2025-03-12|2025-03-12 16:29:...|    1427032998|\n",
      "|2015-03-22T14:38:39|1646641578622|Jan, 2021|  2025-03-12|2025-03-12 16:29:...|    1427031519|\n",
      "+-------------------+-------------+---------+------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "zmianaFormatu = dataFrame.withColumn(\"unix_timestamp\", unix_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "zmianaFormatu.printSchema()\n",
    "zmianaFormatu.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "965ee74b-2147-4065-a06b-a832afc3a7d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Zmie≈Ñ format zgodnie z klasƒÖ `SimpleDateFormat`**yyyy-MM-dd HH:mm:ss**\n",
    "  * a. Wy≈õwietl schemat i dane ≈ºeby sprawdzicz czy warto≈õci siƒô zmieni≈Çy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9013e7b3-750a-442e-9ac9-15d86b16e303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- unix: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      " |-- current_timestamp: timestamp (nullable = false)\n",
      " |-- unix_timestamp: long (nullable = true)\n",
      " |-- formatted_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "zmianaFormatu.withColumn(\"formatted_timestamp\", unix_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\"))\n",
    "\n",
    "zmianaFormatu.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f035d832-20ce-4a42-900a-e4b1a16c225b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#unix_timestamp\n",
    "tempE = zmianaFormatu.withColumn(\"unix_timestamp\", unix_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "display(tempE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db03d5cd-61eb-4fca-a821-e19a85950b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stw√≥rz nowe kolumny do DataFrame z warto≈õciami year(..), month(..), dayofyear(..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1461557-acfa-490d-80e7-ce453bf7a728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, year: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, month: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, dayofyear: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "#date_format\n",
    "yearDate = zmianaFormatu.withColumn(\"year\", date_format(\"timestamp\", \"yyyy\"))\n",
    "display(yearDate)\n",
    "monthDate = zmianaFormatu.withColumn(\"month\", date_format(\"timestamp\", \"MM\"))\n",
    "display(monthDate)\n",
    "dayofyearDate = zmianaFormatu.withColumn(\"dayofyear\", date_format(\"timestamp\", \"D\"))\n",
    "display(dayofyearDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2c0abf1-1002-423c-8bd0-5b2519dd1411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, date_column: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "#to_date()\n",
    "toDate = zmianaFormatu.withColumn(\"date_column\", to_date(\"Date\", \"MMM, yyyy\"))\n",
    "display(toDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867a2aa1-3f1d-41ed-9d75-dbf309fb3dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, from_unixtime: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+---------+------------+--------------------+--------------+-------------------+-------------------+\n",
      "|          timestamp|         unix|     Date|current_date|   current_timestamp|unix_timestamp|formatted_timestamp|      from_unixtime|\n",
      "+-------------------+-------------+---------+------------+--------------------+--------------+-------------------+-------------------+\n",
      "|2015-03-22T14:13:34|1646641525847|May, 2021|  2025-03-12|2025-03-12 16:50:...|    1427030014|2015-03-22 14:13:34|2015-03-22 14:13:34|\n",
      "|2015-03-22T15:03:18|1646641557555|Mar, 2021|  2025-03-12|2025-03-12 16:50:...|    1427032998|2015-03-22 15:03:18|2015-03-22 15:03:18|\n",
      "|2015-03-22T14:38:39|1646641578622|Jan, 2021|  2025-03-12|2025-03-12 16:50:...|    1427031519|2015-03-22 14:38:39|2015-03-22 14:38:39|\n",
      "+-------------------+-------------+---------+------------+--------------------+--------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "#from_unixtime()\n",
    "fromUnix = zmianaFormatu.withColumn(\"from_unixtime\", from_unixtime(\"unix_timestamp\"))\n",
    "display(fromUnix)\n",
    "fromUnix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebba34dc-f8ad-4c31-9de1-59bf7e86e749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, to_timestamp: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "#to_timestamp()\n",
    "toTimestamp = zmianaFormatu.withColumn(\"to_timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "display(toTimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea929b6-c0cb-49a1-9db7-f9b057fb4de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, to_utc_timestamp: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_utc_timestamp\n",
    "#to_utc_timestamp()\n",
    "toUtcTimestamp = zmianaFormatu.withColumn(\"to_utc_timestamp\", to_utc_timestamp(\"timestamp\", \"UTC\"))\n",
    "display(toUtcTimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9d24a58-521a-4bb2-b03a-a1fac517faed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: string, unix: bigint, Date: string, current_date: date, current_timestamp: timestamp, unix_timestamp: bigint, formatted_timestamp: timestamp, from_utc_timestamp: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "# from_utc_timestamp()\n",
    "fromUtcTimestamp = zmianaFormatu.withColumn(\"from_utc_timestamp\", from_utc_timestamp(\"timestamp\", \"UTC\"))\n",
    "display(fromUtcTimestamp)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3763336721693241,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Daty",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
